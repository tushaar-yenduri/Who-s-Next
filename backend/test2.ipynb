{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5417237c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape (1470, 35)\n",
      "Columns: ['Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome', 'Education', 'EducationField', 'EmployeeCount', 'EmployeeNumber', 'EnvironmentSatisfaction', 'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'Over18', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_df=pd.read_csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n",
    "df=raw_df.copy()\n",
    "\n",
    "print(\"Raw shape\",raw_df.shape)\n",
    "print(\"Columns:\",raw_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecaa4c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmployeeNumber unique? -> True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Age, Attrition, BusinessTravel, DailyRate, Department, DistanceFromHome, Education, EducationField, EmployeeCount, EmployeeNumber, EnvironmentSatisfaction, Gender, HourlyRate, JobInvolvement, JobLevel, JobRole, JobSatisfaction, MaritalStatus, MonthlyIncome, MonthlyRate, NumCompaniesWorked, Over18, OverTime, PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StandardHours, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if EmployeeNumber is unique\n",
    "print(\"EmployeeNumber unique? ->\", raw_df[\"EmployeeNumber\"].is_unique)\n",
    "\n",
    "# If not unique, show duplicates\n",
    "dup_ids = raw_df[raw_df.duplicated(\"EmployeeNumber\", keep=False)].sort_values(\"EmployeeNumber\")\n",
    "dup_ids.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe044f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "missing = raw_df.isna().sum().sort_values(ascending=False)\n",
    "print(missing[missing > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90493ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Human Resources': ['Human Resources', 'Manager'],\n",
       " 'Research & Development': ['Research Scientist',\n",
       "  'Laboratory Technician',\n",
       "  'Manufacturing Director',\n",
       "  'Healthcare Representative',\n",
       "  'Research Director',\n",
       "  'Manager'],\n",
       " 'Sales': ['Sales Executive', 'Manager', 'Sales Representative']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you haven’t already:\n",
    "# df = raw_df.copy()\n",
    "\n",
    "dept_job_map = (\n",
    "    df.groupby(\"Department\")[\"JobRole\"]\n",
    "      .unique()\n",
    "      .apply(list)\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "dept_job_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba47e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Healthcare Representative': ['Research & Development'],\n",
       " 'Human Resources': ['Human Resources'],\n",
       " 'Laboratory Technician': ['Research & Development'],\n",
       " 'Manager': ['Sales', 'Research & Development', 'Human Resources'],\n",
       " 'Manufacturing Director': ['Research & Development'],\n",
       " 'Research Director': ['Research & Development'],\n",
       " 'Research Scientist': ['Research & Development'],\n",
       " 'Sales Executive': ['Sales'],\n",
       " 'Sales Representative': ['Sales']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_dept_map = (\n",
    "    df.groupby(\"JobRole\")[\"Department\"]\n",
    "      .unique()\n",
    "      .apply(list)\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "job_dept_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b272f6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition\n",
      "No     1233\n",
      "Yes     237\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attrition_Flag\n",
      "0    0.838776\n",
      "1    0.161224\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Start from your working copy\n",
    "df = raw_df.copy()\n",
    "\n",
    "# Create numeric target\n",
    "df[\"Attrition_Flag\"] = df[\"Attrition\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "# Quick check of class balance\n",
    "print(df[\"Attrition\"].value_counts())\n",
    "print()\n",
    "print(df[\"Attrition_Flag\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9114a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df shape: (1470, 36)\n",
      "Model df shape: (1470, 32)\n",
      "Dropped columns: ['EmployeeNumber', 'EmployeeCount', 'Over18', 'StandardHours']\n"
     ]
    }
   ],
   "source": [
    "# Columns we don't want as features\n",
    "drop_cols = [\"EmployeeNumber\", \"EmployeeCount\", \"Over18\", \"StandardHours\"]\n",
    "\n",
    "# Create a separate modeling dataframe\n",
    "df_model = df.drop(columns=drop_cols)\n",
    "\n",
    "print(\"Original df shape:\", df.shape)\n",
    "print(\"Model df shape:\", df_model.shape)\n",
    "print(\"Dropped columns:\", drop_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2339bcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1470, 30)\n",
      "y shape: (1470,)\n"
     ]
    }
   ],
   "source": [
    "# Target\n",
    "y = df_model[\"Attrition_Flag\"]\n",
    "\n",
    "# Features: drop original text label + numeric flag\n",
    "X = df_model.drop(columns=[\"Attrition\", \"Attrition_Flag\"])\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69a1c5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']\n",
      "Numeric columns count: 23\n"
     ]
    }
   ],
   "source": [
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "print(\"Numeric columns count:\", len(num_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af0e749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_encoded shape: (1470, 44)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <th>HourlyRate</th>\n",
       "      <th>JobInvolvement</th>\n",
       "      <th>JobLevel</th>\n",
       "      <th>JobSatisfaction</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>...</th>\n",
       "      <th>JobRole_Laboratory Technician</th>\n",
       "      <th>JobRole_Manager</th>\n",
       "      <th>JobRole_Manufacturing Director</th>\n",
       "      <th>JobRole_Research Director</th>\n",
       "      <th>JobRole_Research Scientist</th>\n",
       "      <th>JobRole_Sales Executive</th>\n",
       "      <th>JobRole_Sales Representative</th>\n",
       "      <th>MaritalStatus_Married</th>\n",
       "      <th>MaritalStatus_Single</th>\n",
       "      <th>OverTime_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>1102</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5993</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>279</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5130</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1373</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2090</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>1392</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2909</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>591</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3468</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  DailyRate  DistanceFromHome  Education  EnvironmentSatisfaction  \\\n",
       "0   41       1102                 1          2                        2   \n",
       "1   49        279                 8          1                        3   \n",
       "2   37       1373                 2          2                        4   \n",
       "3   33       1392                 3          4                        4   \n",
       "4   27        591                 2          1                        1   \n",
       "\n",
       "   HourlyRate  JobInvolvement  JobLevel  JobSatisfaction  MonthlyIncome  ...  \\\n",
       "0          94               3         2                4           5993  ...   \n",
       "1          61               2         2                2           5130  ...   \n",
       "2          92               2         1                3           2090  ...   \n",
       "3          56               3         1                3           2909  ...   \n",
       "4          40               3         1                2           3468  ...   \n",
       "\n",
       "   JobRole_Laboratory Technician  JobRole_Manager  \\\n",
       "0                          False            False   \n",
       "1                          False            False   \n",
       "2                           True            False   \n",
       "3                          False            False   \n",
       "4                           True            False   \n",
       "\n",
       "   JobRole_Manufacturing Director  JobRole_Research Director  \\\n",
       "0                           False                      False   \n",
       "1                           False                      False   \n",
       "2                           False                      False   \n",
       "3                           False                      False   \n",
       "4                           False                      False   \n",
       "\n",
       "   JobRole_Research Scientist  JobRole_Sales Executive  \\\n",
       "0                       False                     True   \n",
       "1                        True                    False   \n",
       "2                       False                    False   \n",
       "3                        True                    False   \n",
       "4                       False                    False   \n",
       "\n",
       "   JobRole_Sales Representative  MaritalStatus_Married  MaritalStatus_Single  \\\n",
       "0                         False                  False                  True   \n",
       "1                         False                   True                 False   \n",
       "2                         False                  False                  True   \n",
       "3                         False                   True                 False   \n",
       "4                         False                   True                 False   \n",
       "\n",
       "   OverTime_Yes  \n",
       "0          True  \n",
       "1         False  \n",
       "2          True  \n",
       "3          True  \n",
       "4         False  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoded = pd.get_dummies(\n",
    "    X,\n",
    "    columns=cat_cols,\n",
    "    drop_first=True  # avoids dummy variable trap\n",
    ")\n",
    "\n",
    "print(\"X_encoded shape:\", X_encoded.shape)\n",
    "X_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7adc8389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1176, 44)\n",
      "X_test: (294, 44)\n",
      "y_train: (1176,)\n",
      "y_test: (294,)\n",
      "\n",
      "Train class proportion:\n",
      "Attrition_Flag\n",
      "0    0.838435\n",
      "1    0.161565\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test class proportion:\n",
      "Attrition_Flag\n",
      "0    0.840136\n",
      "1    0.159864\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,      # keeps same Yes/No ratio in both splits\n",
    "    random_state=42  # for reproducibility\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "\n",
    "print(\"\\nTrain class proportion:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTest class proportion:\")\n",
    "print(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "069d3b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[244   3]\n",
      " [ 43   4]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.850     0.988     0.914       247\n",
      "           1      0.571     0.085     0.148        47\n",
      "\n",
      "    accuracy                          0.844       294\n",
      "   macro avg      0.711     0.536     0.531       294\n",
      "weighted avg      0.806     0.844     0.791       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "y_prob = rf.predict_proba(X_test)[:, 1]  # for probability-based UI later\n",
    "\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b33a839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (LogReg):\n",
      " [[195  52]\n",
      " [ 15  32]]\n",
      "\n",
      "Classification report (LogReg):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.929     0.789     0.853       247\n",
      "           1      0.381     0.681     0.489        47\n",
      "\n",
      "    accuracy                          0.772       294\n",
      "   macro avg      0.655     0.735     0.671       294\n",
      "weighted avg      0.841     0.772     0.795       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Pipeline: scaling + logistic regression\n",
    "logreg_clf = Pipeline(steps=[\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"logreg\", LogisticRegression(\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "logreg_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = logreg_clf.predict(X_test)\n",
    "y_prob_lr = logreg_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Confusion matrix (LogReg):\\n\", confusion_matrix(y_test, y_pred_lr))\n",
    "print(\"\\nClassification report (LogReg):\\n\", classification_report(y_test, y_pred_lr, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c43bf335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Threshold: 0.3 ===\n",
      "Confusion matrix:\n",
      " [[147 100]\n",
      " [  7  40]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.955     0.595     0.733       247\n",
      "           1      0.286     0.851     0.428        47\n",
      "\n",
      "    accuracy                          0.636       294\n",
      "   macro avg      0.620     0.723     0.580       294\n",
      "weighted avg      0.848     0.636     0.684       294\n",
      "\n",
      "=== Threshold: 0.4 ===\n",
      "Confusion matrix:\n",
      " [[177  70]\n",
      " [ 10  37]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.947     0.717     0.816       247\n",
      "           1      0.346     0.787     0.481        47\n",
      "\n",
      "    accuracy                          0.728       294\n",
      "   macro avg      0.646     0.752     0.648       294\n",
      "weighted avg      0.850     0.728     0.762       294\n",
      "\n",
      "=== Threshold: 0.5 ===\n",
      "Confusion matrix:\n",
      " [[195  52]\n",
      " [ 15  32]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.929     0.789     0.853       247\n",
      "           1      0.381     0.681     0.489        47\n",
      "\n",
      "    accuracy                          0.772       294\n",
      "   macro avg      0.655     0.735     0.671       294\n",
      "weighted avg      0.841     0.772     0.795       294\n",
      "\n",
      "=== Threshold: 0.6 ===\n",
      "Confusion matrix:\n",
      " [[217  30]\n",
      " [ 19  28]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.919     0.879     0.899       247\n",
      "           1      0.483     0.596     0.533        47\n",
      "\n",
      "    accuracy                          0.833       294\n",
      "   macro avg      0.701     0.737     0.716       294\n",
      "weighted avg      0.850     0.833     0.840       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# probabilities from your logistic regression model\n",
    "y_scores = y_prob_lr  # already computed as predict_proba(... )[:, 1]\n",
    "\n",
    "def evaluate_at_threshold(threshold):\n",
    "    y_pred_custom = (y_scores >= threshold).astype(int)\n",
    "    print(f\"=== Threshold: {threshold} ===\")\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_custom))\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred_custom, digits=3))\n",
    "\n",
    "# Try a few thresholds\n",
    "for t in [0.3, 0.4, 0.5, 0.6]:\n",
    "    evaluate_at_threshold(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00483eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN SET ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.955     0.775     0.856       986\n",
      "           1      0.410     0.811     0.544       190\n",
      "\n",
      "    accuracy                          0.781      1176\n",
      "   macro avg      0.682     0.793     0.700      1176\n",
      "weighted avg      0.867     0.781     0.805      1176\n",
      "\n",
      "Train ROC-AUC: 0.8741646204761397\n",
      "Train PR-AUC (average precision): 0.6871078566404933\n",
      "\n",
      "=== TEST SET ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.929     0.789     0.853       247\n",
      "           1      0.381     0.681     0.489        47\n",
      "\n",
      "    accuracy                          0.772       294\n",
      "   macro avg      0.655     0.735     0.671       294\n",
      "weighted avg      0.841     0.772     0.795       294\n",
      "\n",
      "Test ROC-AUC: 0.8079076578516667\n",
      "Test PR-AUC (average precision): 0.571784487599875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
    "\n",
    "# ---- TRAIN PERFORMANCE ----\n",
    "y_train_pred = logreg_clf.predict(X_train)\n",
    "y_train_prob = logreg_clf.predict_proba(X_train)[:, 1]\n",
    "\n",
    "print(\"=== TRAIN SET ===\")\n",
    "print(classification_report(y_train, y_train_pred, digits=3))\n",
    "print(\"Train ROC-AUC:\", roc_auc_score(y_train, y_train_prob))\n",
    "print(\"Train PR-AUC (average precision):\", average_precision_score(y_train, y_train_prob))\n",
    "\n",
    "# ---- TEST PERFORMANCE (same as before, but with extra metrics) ----\n",
    "y_test_pred = logreg_clf.predict(X_test)\n",
    "y_test_prob = logreg_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n=== TEST SET ===\")\n",
    "print(classification_report(y_test, y_test_pred, digits=3))\n",
    "print(\"Test ROC-AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "print(\"Test PR-AUC (average precision):\", average_precision_score(y_test, y_test_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b646327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV ROC-AUC scores: [0.81893631 0.8466294  0.83719528 0.84443104 0.78120424]\n",
      "Mean ROC-AUC: 0.8256792561303596 ± 0.024276126365144944\n",
      "\n",
      "CV PR-AUC scores: [0.53251943 0.64823391 0.59281821 0.65801815 0.56912606]\n",
      "Mean PR-AUC: 0.600143155571167 ± 0.04743610269797584\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "roc_scores = cross_val_score(\n",
    "    logreg_clf,\n",
    "    X_encoded,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"roc_auc\"\n",
    ")\n",
    "\n",
    "pr_scores = cross_val_score(\n",
    "    logreg_clf,\n",
    "    X_encoded,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"average_precision\"\n",
    ")\n",
    "\n",
    "print(\"CV ROC-AUC scores:\", roc_scores)\n",
    "print(\"Mean ROC-AUC:\", roc_scores.mean(), \"±\", roc_scores.std())\n",
    "\n",
    "print(\"\\nCV PR-AUC scores:\", pr_scores)\n",
    "print(\"Mean PR-AUC:\", pr_scores.mean(), \"±\", pr_scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffdcd34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LOGREG =====\n",
      "---- TRAIN ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.955     0.775     0.856       986\n",
      "           1      0.410     0.811     0.544       190\n",
      "\n",
      "    accuracy                          0.781      1176\n",
      "   macro avg      0.682     0.793     0.700      1176\n",
      "weighted avg      0.867     0.781     0.805      1176\n",
      "\n",
      "Train ROC-AUC: 0.8741646204761397\n",
      "Train PR-AUC : 0.6871078566404933\n",
      "\n",
      "---- TEST ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.929     0.789     0.853       247\n",
      "           1      0.381     0.681     0.489        47\n",
      "\n",
      "    accuracy                          0.772       294\n",
      "   macro avg      0.655     0.735     0.671       294\n",
      "weighted avg      0.841     0.772     0.795       294\n",
      "\n",
      "Test ROC-AUC: 0.8079076578516667\n",
      "Test PR-AUC : 0.571784487599875\n",
      "\n",
      "===== RF =====\n",
      "---- TRAIN ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.944     0.895     0.919       986\n",
      "           1      0.570     0.726     0.639       190\n",
      "\n",
      "    accuracy                          0.867      1176\n",
      "   macro avg      0.757     0.810     0.779      1176\n",
      "weighted avg      0.884     0.867     0.874      1176\n",
      "\n",
      "Train ROC-AUC: 0.9007259528130671\n",
      "Train PR-AUC : 0.6817145101489207\n",
      "\n",
      "---- TEST ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.913     0.850     0.881       247\n",
      "           1      0.422     0.574     0.486        47\n",
      "\n",
      "    accuracy                          0.806       294\n",
      "   macro avg      0.667     0.712     0.683       294\n",
      "weighted avg      0.835     0.806     0.818       294\n",
      "\n",
      "Test ROC-AUC: 0.770264449995693\n",
      "Test PR-AUC : 0.39233454170712556\n",
      "\n",
      "===== GB =====\n",
      "---- TRAIN ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.956     1.000     0.978       986\n",
      "           1      1.000     0.763     0.866       190\n",
      "\n",
      "    accuracy                          0.962      1176\n",
      "   macro avg      0.978     0.882     0.922      1176\n",
      "weighted avg      0.963     0.962     0.960      1176\n",
      "\n",
      "Train ROC-AUC: 0.9892548307889398\n",
      "Train PR-AUC : 0.9690450983347355\n",
      "\n",
      "---- TEST ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.864     0.976     0.916       247\n",
      "           1      0.600     0.191     0.290        47\n",
      "\n",
      "    accuracy                          0.850       294\n",
      "   macro avg      0.732     0.584     0.603       294\n",
      "weighted avg      0.822     0.850     0.816       294\n",
      "\n",
      "Test ROC-AUC: 0.8104918597639763\n",
      "Test PR-AUC : 0.4772454905104135\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "# ----- define models -----\n",
    "models = {}\n",
    "\n",
    "# 1) Logistic Regression (with scaling)\n",
    "models[\"logreg\"] = Pipeline(steps=[\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 2) Random Forest (regularized to reduce overfitting)\n",
    "models[\"rf\"] = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=20,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3) Gradient Boosting (regularized)\n",
    "models[\"gb\"] = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ----- helper for evaluation -----\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\n===== {name.upper()} =====\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Train\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "    print(\"---- TRAIN ----\")\n",
    "    print(classification_report(y_train, y_train_pred, digits=3))\n",
    "    print(\"Train ROC-AUC:\", roc_auc_score(y_train, y_train_prob))\n",
    "    print(\"Train PR-AUC :\", average_precision_score(y_train, y_train_prob))\n",
    "    \n",
    "    # Test\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    print(\"\\n---- TEST ----\")\n",
    "    print(classification_report(y_test, y_test_pred, digits=3))\n",
    "    print(\"Test ROC-AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "    print(\"Test PR-AUC :\", average_precision_score(y_test, y_test_prob))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ----- fit all models and store them -----\n",
    "fitted_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    fitted_models[name] = evaluate_model(\n",
    "        name, model,\n",
    "        X_train, y_train,\n",
    "        X_test, y_test\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed0459ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: {'clf__C': 0.1}\n",
      "Best CV PR-AUC: 0.6117914978973933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "pipe_logreg = Pipeline(steps=[\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid_lr = {\n",
    "    \"clf__C\": [1.0, 0.5, 0.25, 0.1]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_lr = GridSearchCV(\n",
    "    pipe_logreg,\n",
    "    param_grid_lr,\n",
    "    cv=cv,\n",
    "    scoring=\"average_precision\",  # focus on minority-class quality\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_lr.fit(X_encoded, y)\n",
    "\n",
    "print(\"Best C:\", grid_lr.best_params_)\n",
    "print(\"Best CV PR-AUC:\", grid_lr.best_score_)\n",
    "logreg_best = grid_lr.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de7e97d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF params: {'max_depth': 4, 'max_features': 0.5, 'min_samples_leaf': 10}\n",
      "Best RF CV PR-AUC: 0.5459519354601203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "param_grid_rf = {\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_samples_leaf\": [10, 20, 50],\n",
    "    \"max_features\": [\"sqrt\", 0.5]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    rf_base,\n",
    "    param_grid_rf,\n",
    "    cv=cv,\n",
    "    scoring=\"average_precision\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_rf.fit(X_encoded, y)\n",
    "\n",
    "print(\"Best RF params:\", grid_rf.best_params_)\n",
    "print(\"Best RF CV PR-AUC:\", grid_rf.best_score_)\n",
    "rf_best = grid_rf.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16c15a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GB params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 150, 'subsample': 0.7}\n",
      "Best GB CV PR-AUC: 0.6079954523858395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_base = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "param_grid_gb = {\n",
    "    \"n_estimators\": [50, 100, 150],\n",
    "    \"learning_rate\": [0.1, 0.05, 0.02],\n",
    "    \"max_depth\": [2, 3],\n",
    "    \"subsample\": [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_gb = GridSearchCV(\n",
    "    gb_base,\n",
    "    param_grid_gb,\n",
    "    cv=cv,\n",
    "    scoring=\"average_precision\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_gb.fit(X_encoded, y)\n",
    "\n",
    "print(\"Best GB params:\", grid_gb.best_params_)\n",
    "print(\"Best GB CV PR-AUC:\", grid_gb.best_score_)\n",
    "gb_best = grid_gb.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36db3746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LOGREG (BEST) =====\n",
      "---- TRAIN ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.949     0.771     0.851       986\n",
      "           1      0.397     0.784     0.527       190\n",
      "\n",
      "    accuracy                          0.773      1176\n",
      "   macro avg      0.673     0.778     0.689      1176\n",
      "weighted avg      0.860     0.773     0.798      1176\n",
      "\n",
      "Train ROC-AUC: 0.8614764599124585\n",
      "Train PR-AUC : 0.6790759882552997\n",
      "\n",
      "---- TEST ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.944     0.822     0.879       247\n",
      "           1      0.443     0.745     0.556        47\n",
      "\n",
      "    accuracy                          0.810       294\n",
      "   macro avg      0.694     0.783     0.717       294\n",
      "weighted avg      0.864     0.810     0.827       294\n",
      "\n",
      "Test ROC-AUC: 0.8105779998277199\n",
      "Test PR-AUC : 0.5868224559643914\n",
      "\n",
      "===== RF (BEST) =====\n",
      "---- TRAIN ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.939     0.910     0.924       986\n",
      "           1      0.597     0.695     0.642       190\n",
      "\n",
      "    accuracy                          0.875      1176\n",
      "   macro avg      0.768     0.802     0.783      1176\n",
      "weighted avg      0.884     0.875     0.879      1176\n",
      "\n",
      "Train ROC-AUC: 0.9053165367780506\n",
      "Train PR-AUC : 0.7234949892716649\n",
      "\n",
      "---- TEST ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.907     0.866     0.886       247\n",
      "           1      0.431     0.532     0.476        47\n",
      "\n",
      "    accuracy                          0.813       294\n",
      "   macro avg      0.669     0.699     0.681       294\n",
      "weighted avg      0.831     0.813     0.821       294\n",
      "\n",
      "Test ROC-AUC: 0.7758635541390301\n",
      "Test PR-AUC : 0.4064000487020029\n",
      "\n",
      "===== GB (BEST) =====\n",
      "---- TRAIN ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.935     0.995     0.964       986\n",
      "           1      0.961     0.642     0.770       190\n",
      "\n",
      "    accuracy                          0.938      1176\n",
      "   macro avg      0.948     0.819     0.867      1176\n",
      "weighted avg      0.939     0.938     0.933      1176\n",
      "\n",
      "Train ROC-AUC: 0.9546012597416461\n",
      "Train PR-AUC : 0.8974805906941393\n",
      "\n",
      "---- TEST ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.870     0.972     0.918       247\n",
      "           1      0.611     0.234     0.338        47\n",
      "\n",
      "    accuracy                          0.854       294\n",
      "   macro avg      0.740     0.603     0.628       294\n",
      "weighted avg      0.828     0.854     0.825       294\n",
      "\n",
      "Test ROC-AUC: 0.8121285209751056\n",
      "Test PR-AUC : 0.5202695191219491\n"
     ]
    }
   ],
   "source": [
    "# Using the best estimators you just got:\n",
    "logreg_best = grid_lr.best_estimator_\n",
    "rf_best = grid_rf.best_estimator_\n",
    "gb_best = grid_gb.best_estimator_\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
    "\n",
    "def eval_on_split(name, model):\n",
    "    print(f\"\\n===== {name} (BEST) =====\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Train\n",
    "    y_tr_pred = model.predict(X_train)\n",
    "    y_tr_prob = model.predict_proba(X_train)[:, 1]\n",
    "    print(\"---- TRAIN ----\")\n",
    "    print(classification_report(y_train, y_tr_pred, digits=3))\n",
    "    print(\"Train ROC-AUC:\", roc_auc_score(y_train, y_tr_prob))\n",
    "    print(\"Train PR-AUC :\", average_precision_score(y_train, y_tr_prob))\n",
    "\n",
    "    # Test\n",
    "    y_te_pred = model.predict(X_test)\n",
    "    y_te_prob = model.predict_proba(X_test)[:, 1]\n",
    "    print(\"\\n---- TEST ----\")\n",
    "    print(classification_report(y_test, y_te_pred, digits=3))\n",
    "    print(\"Test ROC-AUC:\", roc_auc_score(y_test, y_te_prob))\n",
    "    print(\"Test PR-AUC :\", average_precision_score(y_test, y_te_prob))\n",
    "\n",
    "    return model\n",
    "\n",
    "logreg_best = eval_on_split(\"LOGREG\", logreg_best)\n",
    "rf_best = eval_on_split(\"RF\", rf_best)\n",
    "gb_best = eval_on_split(\"GB\", gb_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03caaa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final tuned models\n",
    "logreg_best = grid_lr.best_estimator_\n",
    "rf_best = grid_rf.best_estimator_\n",
    "gb_best = grid_gb.best_estimator_\n",
    "\n",
    "fitted_models = {\n",
    "    \"logreg\": logreg_best,   # main / recommended\n",
    "    \"rf\": rf_best,           # alternative\n",
    "    \"gb\": gb_best            # alternative + part of ensemble\n",
    "}\n",
    "\n",
    "RISK_THRESHOLD = 0.4  # fixed, not editable in UI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b123e981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENSEMBLE (LOGREG + RF + GB) – TEST SET ===\n",
      "ROC-AUC: 0.8147988629511586\n",
      "PR-AUC : 0.5561941630713517\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.924     0.834     0.877       247\n",
      "           1      0.423     0.638     0.508        47\n",
      "\n",
      "    accuracy                          0.803       294\n",
      "   macro avg      0.673     0.736     0.693       294\n",
      "weighted avg      0.844     0.803     0.818       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
    "\n",
    "def ensemble_proba(fitted_models, X):\n",
    "    probs = []\n",
    "    for name, model in fitted_models.items():\n",
    "        p = model.predict_proba(X)[:, 1]\n",
    "        probs.append(p)\n",
    "    probs = np.vstack(probs)          # (n_models, n_samples)\n",
    "    return probs.mean(axis=0)         # (n_samples,)\n",
    "\n",
    "def classify_from_proba(probs, threshold=RISK_THRESHOLD):\n",
    "    return (probs >= threshold).astype(int)\n",
    "\n",
    "ensemble_test_prob = ensemble_proba(fitted_models, X_test)\n",
    "ensemble_test_pred = classify_from_proba(ensemble_test_prob)\n",
    "\n",
    "print(\"=== ENSEMBLE (LOGREG + RF + GB) – TEST SET ===\")\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, ensemble_test_prob))\n",
    "print(\"PR-AUC :\", average_precision_score(y_test, ensemble_test_prob))\n",
    "print(\"\\nClassification report:\\n\",\n",
    "      classification_report(y_test, ensemble_test_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5582e140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all models.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(logreg_best, \"model_logreg_best.joblib\")\n",
    "joblib.dump(rf_best, \"model_rf_best.joblib\")\n",
    "joblib.dump(gb_best, \"model_gb_best.joblib\")\n",
    "\n",
    "print(\"Saved all models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d997c628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved feature columns: 44\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# X_encoded is your final training feature matrix\n",
    "feature_cols = X_encoded.columns\n",
    "\n",
    "joblib.dump(feature_cols, \"model_feature_columns.joblib\")\n",
    "print(\"Saved feature columns:\", len(feature_cols))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
